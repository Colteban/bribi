# -*- coding: utf-8 -*-
import os, json, hashlib, datetime, re, pathlib, yaml, time, urllib.parse, unicodedata
import feedparser, trafilatura, requests
from collections import defaultdict
from bs4 import BeautifulSoup
from pathlib import Path


import google.generativeai as genai
GEMINI_KEY = os.getenv("GEMINI_API_KEY")
if GEMINI_KEY:
    genai.configure(api_key=GEMINI_KEY)
    GEMINI_MODEL = genai.GenerativeModel("gemini-1.5-flash")
else:
    GEMINI_MODEL = None

GEMINI_BUDGET = int(os.getenv("GEMINI_BUDGET", "12"))  # m√°x. llamados IA por corrida (puedes poner 10 si quieres)



# Detecta la ra√≠z del repo (GitHub Actions expone GITHUB_WORKSPACE)
REPO_ROOT = pathlib.Path(os.getenv("GITHUB_WORKSPACE") or pathlib.Path(__file__).resolve().parents[1]).resolve()
CONTENT = REPO_ROOT / "src" / "content" / "blog"
DATA = REPO_ROOT / "data"; DATA.mkdir(parents=True, exist_ok=True)
SEEN = DATA / "seen.json"
FEEDS_FILE = DATA / "feeds.txt"

print("USANDO RAIZ:", REPO_ROOT)
print("CONTENIDO EN:", CONTENT)
print("FEEDS_FILE:", FEEDS_FILE)


MAX_NEW = int(os.getenv("MAX_NEW", "8"))
TIMEOUT = 15

STYLES = {
    # Estilo cr√≠tico/organizado para top CO y WORLD y silencios:
    "critico": (
        "Organiza y narra cronol√≥gica o tem√°ticamente. "
        "Incluye an√°lisis cr√≠tico (causas, efectos, riesgos, oportunidades) y contrastaci√≥n. "
        "Menciona la fuente principal una sola vez de forma profesional (p. ej., 'seg√∫n <dominio>')."
    ),
    # Emprendimiento:
    "emprende": (
        "Organiza con foco en emprendedores/pyme: costos, tr√°mites, beneficios, pasos. "
        "Menciona la fuente principal una sola vez."
    ),
    # Finanzas:
    "finanzas": (
        "Organiza tem√°ticamente: tasas, liquidez, cartera, riesgo, indicadores, efectos para comercios. "
        "Menciona la fuente principal una sola vez."
    ),
    # Econom√≠a:
    "economia": (
        "Organiza tem√°ticamente: crecimiento, empleo, inflaci√≥n, pol√≠tica monetaria/fiscal. "
        "Menciona la fuente principal una sola vez."
    ),
    # Pagos/fintech:
    "pagos": (
        "Organiza tem√°ticamente: rails (PIX, UPI, CBDC), redes (Visa, Mastercard, Redeban), wallets (Nequi), liquidaci√≥n, interoperabilidad. "
        "Menciona la fuente principal una sola vez."
    ),
    # Proyectos/Inversiones:
    "proyectos": (
        "Organiza tem√°ticamente: monto, fuente de recursos, cronograma, actores, riesgos, estado. "
        "Menciona la fuente principal una sola vez."
    ),
    # Bolet√≠n de respaldo:
    "boletin": (
        "Bolet√≠n ejecutivo: 4‚Äì6 frases claras y 3 bullets accionables. "
        "Menciona la fuente principal una sola vez."
    ),
}


# extraer canonical y og:image
def extract_meta(url, html):
    soup = BeautifulSoup(html, "html.parser")
    canonical = soup.find("link", rel=lambda v: v and "canonical" in v.lower())
    ogimg = soup.find("meta", property="og:image")
    return (
        canonical.get("href") if canonical else url,
        ogimg.get("content") if ogimg else ""
    )


seen = set()
if SEEN.exists():
    try:
        seen = set(json.loads(SEEN.read_text(encoding="utf-8")))
    except Exception:
        seen = set()

def slugify(s):
    # quita acentos/diacr√≠ticos (√° -> a, √± -> n)
    s = unicodedata.normalize("NFKD", s)
    s = "".join(c for c in s if not unicodedata.combining(c))
    s = re.sub(r"[^a-zA-Z0-9\- ]+", "", s)
    s = s.strip().lower().replace(" ", "-")
    return re.sub(r"-+", "-", s)[:90]

SEP_PAT = re.compile(r"\s*(\||-|‚Äî|‚Äì|¬∑|‚Ä¢|:|::)\s*")

SEP_PAT = re.compile(r"\s*(\||-|‚Äî|‚Äì|¬∑|‚Ä¢|:|::)\s*")

def clean_title(raw):
    if not raw:
        return "Actualizaci√≥n"
    raw = re.sub(r"^\s*opini[o√≥]n\s*:\s*", "", raw, flags=re.I)
    raw = re.sub(r"\s*(\||‚Äî|‚Äì|:|¬∑|‚Ä¢)\s*.*$", "", raw)  # corta cola tras separadores
    raw = re.sub(r"\s+", " ", raw).strip()
    if len(raw) > 65:
        cut = raw[:65]
        cut = re.sub(r"\s+\S*$", "", cut)  # no cortar palabra
        raw = cut
    return raw[0].upper() + raw[1:]



# Frases/fragmentos a eliminar (en min√∫sculas)
# ===== Limpieza potente =====
MONTHS = r"(enero|febrero|marzo|abril|mayo|junio|julio|agosto|septiembre|setiembre|octubre|noviembre|diciembre)"

STOP_PHRASES = [
    "compartir el c√≥digo iframe se ha copiado en el portapapeles",
    "publicidad", "anuncio", "s√≠guenos en", "newsletter", "suscr√≠bete", "suscribete",
    "le puede interesar", "tambi√©n le puede interesar", "te puede interesar",
    "haz clic aqu√≠", "haga clic aqu√≠", "ver m√°s", "ver mas", "leer m√°s", "leer mas",
    "contin√∫e leyendo", "pol√≠tica de tratamiento de datos", "t√©rminos y condiciones",
    "comentarios", "deja tu comentario", "men√∫", "buscar"
]

STOP_REGEXES = [
    r"\bcreative\s*commons\b", r"\bcc\s*by(-| )?nc(-| )?sa\b", r"\blicencia\b",
    r"esta\s+revista\s+est[√°a]\s+autorizada",
    r"el\s+contenido\s+de\s+los\s+art[√≠i]culos\s+es\s+responsabilidad",
    r"no\s+puede\s+ser\s+utilizada\s+con\s+fines\s+comerciales",
    rf"^\s*(lun(es)?|mar(tes)?|mi[e√©]rcoles|jue(ves)?|vie(rnes)?|s[√°a]b(ado)?|dom(ingo)?)\s*,?\s*\d{{1,2}}[./-]\d{{1,2}}[./-]\d{{2,4}}",
    rf"^\s*\d{{1,2}}\s+de\s+{MONTHS}\s+de\s+\d{{4}}\s*$",
    r"^\s*\d{1,2}/\d{1,2}/\d{2,4}\s*$",
    r"^\s*\d{1,2}:\d{2}\s*$",
    r"^\s*lo\s+m[a√°]s\s+visto\s*$",
]

CUT_AFTER_MARKERS = [
    "lo m√°s visto", "referencias", "bibliograf√≠a", "bibliografia", "licencia",
    "copyright", "nota del editor", "cr√©ditos", "creditos"
]

def _looks_like_js(line: str) -> bool:
    l = line.strip()
    if l.startswith(("//", "/*", "*")): return True
    if "$(" in l or "function(" in l or "var " in l or "let " in l or "const " in l: return True
    if "</script>" in l.lower() or "<script" in l.lower(): return True
    symbols = sum(ch in "{}[]();$<>=*#|%\\" for ch in l)
    return symbols > max(6, len(l)//6)

def clean_text(txt: str) -> str:
    if not txt: return ""
    t = unicodedata.normalize("NFKC", txt)
    t = t.replace("‚Ä¶", "...")
    t = re.sub(r"\.{3,}", ".", t)
    t = re.sub(r"[ \t]+", " ", t)
    t = re.sub(r"\n{3,}", "\n\n", t)

    out, seen = [], set()
    cutting = False
    for raw in t.splitlines():
        ln = raw.strip()
        if not ln or len(ln) < 3: 
            continue
        low = ln.lower()

        if cutting:
            continue
        if any(m in low for m in CUT_AFTER_MARKERS):
            cutting = True
            continue

        if _looks_like_js(ln): 
            continue
        if any(p in low for p in STOP_PHRASES): 
            continue
        if any(re.search(rx, low) for rx in STOP_REGEXES): 
            continue

        norm = re.sub(r'[‚Äú‚Äù"¬´¬ª]+', "", low)
        if norm in seen: 
            continue
        seen.add(norm)
        out.append(ln)

    body = "\n\n".join(out).strip()
    body = re.sub(r"\.{3,}", ".", body)
    return body

def quality_ok(text: str) -> bool:
    if not text:
        return False
    if len(text) < 500:  # exige un m√≠nimo real
        return False
    # ratio de l√≠neas √∫nicas (evita repeticiones/ruido)
    lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
    uniq = len(set(lines))
    return (uniq / max(1, len(lines))) >= 0.7


def domain_of(url: str) -> str:
    try:
        import urllib.parse
        netloc = urllib.parse.urlparse(url).netloc.lower()
        return netloc.split(":")[0]
    except Exception:
        return ""

BIG_MEDIA = {
    "semana.com","eltiempo.com","elespectador.com","larepublica.co",
    "portafolio.co","wradio.com.co","caracol.com.co","bluradio.com",
}

CO_HINTS = ["colombia","bogot√°","bogota","medell√≠n","medellin","antioquia","cali",".co/"]
LATAM_COUNTRIES = [
    "argentina","bolivia","brasil","brasil","chile","colombia","costa rica","cuba","ecuador",
    "el salvador","guatemala","honduras","m√©xico","mexico","nicaragua","panam√°","panama","paraguay",
    "per√∫","peru","rep√∫blica dominicana","uruguay","venezuela"
]

def infer_region(url: str, text: str) -> str:
    u = url.lower(); t = text[:800].lower()
    if u.endswith(".co") or any(h in u for h in CO_HINTS) or any(h in t for h in CO_HINTS):
        return "CO"
    if any(c in u for c in LATAM_COUNTRIES) or any(c in t for c in LATAM_COUNTRIES):
        return "LATAM"
    return "WORLD"

TOPIC_KEYWORDS = {
    "economia":  ["econom√≠a","economia","inflaci√≥n","pib","crecimiento","empleo","banrep","minhacienda","macro"],
    "finanzas":  ["tasa","cr√©dito","cartera","banca","liquidez","usura","morosidad","riesgo","financiero"],
    "pagos":     ["pagos","pos","qr","billetera","pse","adquirente","pasarela","pix","upi","cbdc","cdbc","visa","mastercard","redeban","nequi","swift"],
    "emprende":  ["emprend","startup","pyme","c√°mara de comercio","camara de comercio","registro mercantil","rueda de negocios","aceleradora","incubadora"],
    "proyectos": ["proyecto","inversi√≥n","inversion","capex","obra","megaproyecto","concesi√≥n","ani","fdn","publica privada","app "],
    "judicial":  ["corte","tribunal","sentencia","tutela","demanda","sanci√≥n","sancion","proceso judicial"],
    "tecnologia":["tecnolog√≠a","tecnologia","ia","inteligencia artificial","nube","cloud","ciberseguridad","blockchain"],
    "cripto":    ["bitcoin","btc","cripto","crypto","ethereum","eth","stablecoin"],
    "politica":  ["congreso","decreto","ley","reglamenta","reforma","pol√≠tica p√∫blica","politica publica"],
    "negocios":  ["negocio","adquisici√≥n","adquisicion","fusiones","alianza","joint venture","expansi√≥n","expansion"],
    "cooperativas":["cooperativa","mutual","solidaria","finanzas solidarias"],
    "comunidades":["comunidad","local","barrio","asociaci√≥n","asociacion"],
}

def infer_topics(url: str, text: str) -> list:
    u = url.lower(); t = text.lower()
    found = set()
    for k, kws in TOPIC_KEYWORDS.items():
        if any(w in u for w in kws) or any(w in t for w in kws):
            found.add(k)
    if not found:
        found.add("economia")
    return list(found)

def pretty_tags(topics: list, region: str) -> list:
    mapping = {
        "economia":"Econom√≠a","finanzas":"Finanzas","pagos":"Pagos","emprende":"Emprendimiento",
        "proyectos":"Proyectos","judicial":"Judicial","tecnologia":"Tecnolog√≠a","cripto":"Cripto",
        "politica":"Pol√≠tica","negocios":"Negocios","cooperativas":"Cooperativas","comunidades":"Comunidades",
    }
    tags = [mapping.get(t, t.title()) for t in topics]
    if region == "CO": tags.append("Colombia")
    elif region == "LATAM": tags.append("Am√©rica Latina")
    else: tags.append("Mundo")
    # quita duplicados preservando orden
    seen=set(); out=[]
    for x in tags:
        if x not in seen:
            seen.add(x); out.append(x)
    return out

def looks_like_directory(text: str) -> bool:
    """True si parece listado/√≠ndice (no art√≠culo)."""
    if not text: 
        return True
    lines = [ln for ln in text.splitlines() if ln.strip()]
    if len(lines) < 3: 
        return True
    short = sum(1 for ln in lines if len(ln) < 60 and not ln.endswith("."))
    if short / max(1, len(lines)) > 0.6: 
        return True
    verbs = [" es ", " son ", " fue ", " fueron ", " tiene ", " tienen ", " anunci√≥", " anuncia", " public√≥", " regula", " aprueba", " modifica"]
    if sum(1 for v in verbs if v in " " + text.lower() + " ") < 1: 
        return True
    return False

def h(text): return hashlib.sha1(text.encode("utf-8", errors="ignore")).hexdigest()

def get_html(url):
    try:
        r = requests.get(url, timeout=TIMEOUT, headers={"User-Agent":"Mozilla/5.0"})
        if r.status_code == 200:
            return r.text, r.url
    except Exception:
        return None, url
    return None, url


def discover_feed(url):
    # si ya parece feed, √∫salo tal cual
    if any(x in url for x in ["/feed", ".xml", "rss", "atom"]):
        return url
    html, final_url = get_html(url)
    if not html:
        return None
    soup = BeautifulSoup(html, "html.parser")
    for link in soup.find_all("link", rel=lambda v: v and "alternate" in v.lower()):
        t = (link.get("type") or "").lower()
        if any(k in t for k in ("rss", "atom", "xml")):
            href = link.get("href")
            if not href:
                continue
            return urllib.parse.urljoin(final_url, href)
    return None


def extract_article(url):
    try:
        downloaded = trafilatura.fetch_url(url, no_ssl=True)
        text = trafilatura.extract(
            downloaded,
            include_comments=False,
            include_tables=False,
            favor_recall=True,
            target_language="es",
        ) or ""
        return text.strip()
    except Exception:
        return ""

def summarize_with_gemini(text, url, style="critico", main_domain=None):
    if not GEMINI_MODEL or not text:
        return None
    style_rules = STYLES.get(style, STYLES["critico"])
    src = main_domain or (url.split("//")[-1].split("/")[0])

    prompt = f"""
Eres editor econ√≥mico para comercios en Colombia/LATAM.
Reescribe SIN copiar textual. Tono neutro. Entrega JSON: title, summary, article.

Estilo: {style_rules}

Reglas:
- No incluyas licencias ni disclaimers (Creative Commons, etc.).
- No pongas fechas/horas en t√≠tulo ni summary.
- Evita 'OPINI√ìN:' en el t√≠tulo; usa tono informativo.
- Evita citas largas; una corta si aporta valor.
- T√≠tulo: 6‚Äì12 palabras, sin ‚Äú| Nombre del medio‚Äù.
- Summary: 150‚Äì250 palabras, 3‚Äì4 frases, claras y accionables.
- Article: 400‚Äì700 palabras, 5‚Äì8 p√°rrafos; cierra con:
  "Qu√© vigilar" (3 bullets accionables).
- Cita la fuente principal UNA vez de forma profesional, por ejemplo: "seg√∫n {src}".
- No inventes datos; si falta info, dilo sin suponer.
- Espa√±ol (Colombia). Fuente: {url}

TEXTO LIMPIO (parcial si es largo):
{text[:9000]}
"""
    try:
        r = GEMINI_MODEL.generate_content(prompt)
        raw = (r.text or "").strip()
        import json
        i, j = raw.find("{"), raw.rfind("}")
        if i != -1 and j != -1:
            data = json.loads(raw[i:j+1])
            t = (data.get("title") or "").strip()
            s = (data.get("summary") or "").strip()
            a = (data.get("article") or "").strip()
            if len(s) >= 140 and len(a) >= 350:
                return {"title": t, "summary": s, "article": a}
    except Exception:
        return None
    return None


def guess_tags_from_url(url: str) -> list:
    u = url.lower()
    tags = set()
    if "ambitojuridico" in u:
        tags.add("normativa")
        if "tipo-civil" in u: tags.update(["civil"])
        if "derecho-mercantil" in u or "comercial" in u: tags.update(["comercial","pyme"])
    if "camaramedellin" in u: tags.update(["pyme","empresas"])
    if "banrep" in u or "minhacienda" in u: tags.add("econom√≠a")
    if "pagos" in u or "pos" in u or "billetera" in u: tags.add("pagos")
    return list(tags) or ["econom√≠a"]


def parse_feed(feed_url, limit=8):
    items = []
    d = feedparser.parse(feed_url)
    for e in d.entries[:limit]:
        title = (e.get("title") or "").strip()
        link = e.get("link") or ""
        if not title or not link: continue
        items.append((title, link))
    return items

def discover_articles_from_home(home_url, limit=5):
    html, final_url = get_html(home_url)
    if not html:
        return []
    base = urllib.parse.urlparse(final_url).netloc
    soup = BeautifulSoup(html, "html.parser")
    links = []
    for a in soup.find_all("a", href=True):
        href = urllib.parse.urljoin(final_url, a["href"])
        u = urllib.parse.urlparse(href)
        if u.netloc != base:
            continue
        if any(seg in href.lower() for seg in ["econom", "finan", "negocio", "notic", "colombia"]):
            title = a.get_text(strip=True)[:120] or u.path
            links.append((title, href))
        if len(links) >= limit:
            break
    return links


def write_md(title, link, body, og_image="", ai=None, status="draft"):
    today = datetime.date.today().isoformat()

    # t√≠tulo (SEO, usando IA si hay)
    title = clean_title(ai["title"] if ai and ai.get("title") else title)

    # summary limpio (sin cortar palabra y sin "..." extra)
    if ai and ai.get("summary"):
        summary = ai["summary"].strip()
    else:
        trimmed = body[:300]
        trimmed = re.sub(r"\s+\S*$", "", trimmed)   # corta en palabra
        summary = trimmed if len(body) <= 300 else trimmed + "‚Ä¶"

    # contenido (IA si hay, si no el body limpio)
    article_md = ai["article"] if ai and ai.get("article") else body
    if not article_md or len(article_md) < 200:
        return

    # slug √∫nico
    base_slug = slugify(title)
    slug = f"{today}-{base_slug}"
    p = CONTENT / f"{slug}.md"
    if p.exists():
        slug = f"{today}-{base_slug}-{h(title)[:6]}"
        p = CONTENT / f"{slug}.md"

    # tags bonitos desde topics/region (si no hay IA, usa defaults)
    topics = (ai.get("topics") if ai and ai.get("topics") else [])
    region = (ai.get("region") if ai and ai.get("region") else "WORLD")
    auto_tags = pretty_tags(topics, region)

    # canonical siempre al link de la fuente (ya lo calculaste al procesar)
    canonical = link

    # FRONTMATTER ‚Äî ¬°ojo comas y llaves!
    fm = {
        "title": title,
        "description": summary,
        "pubDate": today,
        "tags": auto_tags,
        "status": status,
        "risk": "bajo",
        "action": "Evaluar impacto en comisiones/operaci√≥n.",
        "sources": [{"name": "Fuente", "url": link}],
        "canonicalUrl": canonical,
    }
    if og_image:
        fm["image"] = {"src": og_image, "alt": title}

    md = "---\n" + yaml.safe_dump(fm, allow_unicode=True, sort_keys=False) + "---\n" + article_md.strip()
    p.write_text(md, encoding="utf-8")

def _parse_feed_items(xml_text: str, base_url: str):
    """Devuelve [(title, link), ...] desde un XML RSS/Atom."""
    items = []
    soup = BeautifulSoup(xml_text, "xml")

    # RSS <item>
    for it in soup.find_all("item"):
        title = it.title.get_text(strip=True) if it.title else ""
        link = ""
        if it.link:
            # algunos RSS traen <link> como texto
            link = it.link.get_text(strip=True)
        if not link and it.guid and (it.guid.get("isPermaLink") in (True, "true")):
            link = it.guid.get_text(strip=True)
        if title and link:
            link = urllib.parse.urljoin(base_url, link)
            items.append((title, link))

    # Atom <entry>
    for e in soup.find_all("entry"):
        title = e.title.get_text(strip=True) if e.title else ""
        link = ""
        for lk in e.find_all("link"):
            rel = lk.get("rel", "")
            if not rel or "alternate" in rel:
                href = lk.get("href")
                if href:
                    link = urllib.parse.urljoin(base_url, href)
                    break
        if not link and e.id:
            link = e.id.get_text(strip=True)
        if title and link:
            items.append((title, link))

    return items


def _discover_feed_url(page_html: str, page_url: str):
    """Si te pasan una p√°gina HTML, intenta encontrar el <link rel='alternate' ...> del feed."""
    soup = BeautifulSoup(page_html, "html.parser")
    # busca RSS o Atom
    link = soup.find("link", rel=lambda r: r and "alternate" in r,
                     type=lambda t: t and ("rss" in t or "atom" in t or "xml" in t))
    if link and link.get("href"):
        return urllib.parse.urljoin(page_url, link["href"])
    return None


def discover_candidates(feeds_file_path, max_per_feed=20, overall_limit=200):
    """
    Lee data/feeds.txt, intenta encontrar RSS/Atom para cada URL y devuelve [(title, link), ...].
    - max_per_feed: tope por feed
    - overall_limit: tope global (para no traer miles)
    """
    candidates = []

    # 1) lee las URLs del archivo
    try:
        if hasattr(feeds_file_path, "read_text"):
            lines = feeds_file_path.read_text(encoding="utf-8").splitlines()
        else:
            with open(feeds_file_path, "r", encoding="utf-8") as f:
                lines = f.read().splitlines()
    except Exception:
        lines = []

    urls = [u.strip() for u in lines if u.strip() and not u.strip().startswith("#")]

    # 2) procesa cada URL
    for url in urls:
        html, final_url = get_html(url)
        if not html:
            continue

        # ¬øya es un feed (tiene <rss ...> o <feed ...>)?
        is_xml_like = ("<rss" in html.lower()) or ("<feed" in html.lower())
        feed_xml = None

        if is_xml_like:
            feed_xml = html
            feed_base = final_url
        else:
            # intenta descubrir el <link rel="alternate" type="application/rss+xml|atom+xml">
            feed_url = _discover_feed_url(html, final_url)
            if feed_url:
                xml_text, xml_final = get_html(feed_url)
                if xml_text:
                    feed_xml = xml_text
                    feed_base = xml_final or feed_url

        # si no encontramos feed, intenta parsear como si fuera RSS/Atom de todos modos (algunos sitios devuelven XML con content-type raro)
        if not feed_xml:
            if is_xml_like:
                feed_xml = html
                feed_base = final_url
            else:
                continue  # no hay feed detectable

        # 3) extrae items y acumula
        items = _parse_feed_items(feed_xml, feed_base)
        for t, l in items[:max_per_feed]:
            candidates.append((t, l))

        if len(candidates) >= overall_limit:
            break

    return candidates[:overall_limit]



def run():
    from pathlib import Path            # üëà IMPORT DENTRO DE run() (obligatorio)
    import os, urllib.parse, time
    from collections import defaultdict

    ROOT = Path(__file__).resolve().parents[1]
    CONTENT = ROOT / "src" / "content" / "blog"
    FEEDS_FILE = ROOT / "data" / "feeds.txt"
    print("USANDO RAIZ:", ROOT)
    print("CONTENIDO EN:", CONTENT)
    print("FEEDS_FILE:", FEEDS_FILE)

    CONTENT.mkdir(parents=True, exist_ok=True)

    # 1) Recolector de candidatos
    POOL = []

    # Construye lista (t√≠tulo, link) como ya lo haces
    candidates = discover_candidates(FEEDS_FILE)

    # deduplicaci√≥n b√°sica (si tienes load_seen/h, se usan; si no, fallbacks)
    try:
        seen = load_seen()
    except Exception:
        seen = set()

    for title, link in candidates:
        try:
            key0 = h(title + link)
        except Exception:
            key0 = f"{title}|{link}"

        if key0 in seen:
            continue

        html, final_url = get_html(link)
        canon = final_url
        og_image = ""  # no reutilizamos OG de la fuente

        if html:
            c, _ = extract_meta(final_url, html)  # ignoramos imagen OG
            if c:
                canon = urllib.parse.urljoin(final_url, c)

        raw = extract_article(canon or final_url)
        body = clean_text(raw)
        if not body or len(body) < 200:
            continue
        if not quality_ok(body):
            continue

        dom    = domain_of(canon)
        region = infer_region(canon, body)      # "CO"/"LATAM"/"WORLD"
        topics = infer_topics(canon, body)      # p.ej. ["economia","finanzas"]
        is_big = dom in BIG_MEDIA

        POOL.append({
            "title": title,
            "url": canon,     # guardamos la can√≥nica aqu√≠
            "body": body,
            "domain": dom,
            "region": region,
            "topics": topics,
            "is_big": is_big,
        })

        seen.add(key0)
        time.sleep(0.4)

    if not POOL:
        print("No hubo candidatos v√°lidos en esta corrida.")
        return

    # 2) Selecci√≥n priorizada ‚Üí 12 (o seg√∫n env)
    MAX_NEW = int(os.getenv("MAX_NEW", "12"))
    GEMINI_BUDGET = int(os.getenv("GEMINI_BUDGET", "12"))

    by_region_topic = defaultdict(list)
    by_topic = defaultdict(list)
    non_big_by_topic_CO = defaultdict(list)
    for it in POOL:
        for tp in it["topics"]:
            by_topic[tp].append(it)
            by_region_topic[(it["region"], tp)].append(it)
            if it["region"] == "CO" and not it["is_big"]:
                non_big_by_topic_CO[tp].append(it)

    def pick_one(lst, used):
        for it in lst:
            if it["url"] not in used:
                used.add(it["url"])
                return it
        return None

    selected, used = [], set()

    # (1) 2 temas con m√°s publicaciones en CO (estilo cr√≠tico)
    top_CO_topics = sorted(
        [(tp, len(by_region_topic[("CO", tp)])) for tp in by_topic.keys()],
        key=lambda x: x[1], reverse=True
    )
    for tp, _n in top_CO_topics[:2]:
        it = pick_one(by_region_topic[("CO", tp)], used)
        if it: selected.append(("critico", it))
        if len(selected) >= MAX_NEW: break

    # (2) 1 tema con m√°s publicaciones en WORLD (estilo cr√≠tico)
    if len(selected) < MAX_NEW:
        top_WORLD_topics = sorted(
            [(tp, len(by_region_topic[("WORLD", tp)])) for tp in by_topic.keys()],
            key=lambda x: x[1], reverse=True
        )
        if top_WORLD_topics:
            tp, _n = top_WORLD_topics[0]
            it = pick_one(by_region_topic[("WORLD", tp)], used)
            if it: selected.append(("critico", it))

    # (3) 2 ‚Äúsilencios‚Äù en CO: ‚â•2 notas no-grandes y 0 de grandes (estilo cr√≠tico)
    if len(selected) < MAX_NEW:
        silencios = []
        for tp, lst in non_big_by_topic_CO.items():
            if len(lst) >= 2:
                big_count = sum(1 for x in by_region_topic[("CO", tp)] if x["is_big"])
                if big_count == 0:
                    silencios.append((tp, len(lst)))
        for tp, _ in sorted(silencios, key=lambda x: x[1], reverse=True)[:2]:
            it = pick_one(non_big_by_topic_CO[tp], used)
            if it: selected.append(("critico", it))
            if len(selected) >= MAX_NEW: break

    # (4) 1 Emprendimiento en CO
    if len(selected) < MAX_NEW:
        it = pick_one(by_region_topic.get(("CO","emprende"), []), used)
        if it: selected.append(("emprende", it))

    # (5) 1 Finanzas en CO
    if len(selected) < MAX_NEW:
        it = pick_one(by_region_topic.get(("CO","finanzas"), []), used)
        if it: selected.append(("finanzas", it))

    # (6) 1 Econom√≠a en CO
    if len(selected) < MAX_NEW:
        it = pick_one(by_region_topic.get(("CO","economia"), []), used)
        if it: selected.append(("economia", it))

    # (7) 1 Pagos/fintech (preferencia CO ‚Üí LATAM ‚Üí WORLD)
    if len(selected) < MAX_NEW:
        it = (pick_one(by_region_topic.get(("CO","pagos"), []), used)
              or pick_one(by_region_topic.get(("LATAM","pagos"), []), used)
              or pick_one(by_region_topic.get(("WORLD","pagos"), []), used))
        if it: selected.append(("pagos", it))

    # (8) 1 Proyectos/Inversiones en CO
    if len(selected) < MAX_NEW:
        it = pick_one(by_region_topic.get(("CO","proyectos"), []), used)
        if it: selected.append(("proyectos", it))

    # (9) +2 extras (Tecnolog√≠a CO, Judicial CO) y relleno si faltan (prefiere CO)
    EXTRA_TARGETS = [("tecnologia","CO"), ("judicial","CO")]
    for tp, rg in EXTRA_TARGETS:
        if len(selected) >= MAX_NEW: break
        it = pick_one(by_region_topic.get((rg, tp), []), used)
        if it: selected.append(("critico", it))

    if len(selected) < MAX_NEW:
        resto = [it for it in POOL if it["url"] not in used]
        order = {"CO":0, "LATAM":1, "WORLD":2}
        resto.sort(key=lambda x: order.get(x["region"], 3))
        for it in resto:
            selected.append(("boletin", it))
            used.add(it["url"])
            if len(selected) >= MAX_NEW:
                break

    # 3) Escribir (respetando presupuesto IA)
    calls = 0
    for style, it in selected:
        ai = None
        if calls < GEMINI_BUDGET:
            ai = summarize_with_gemini(it["body"], it["url"], style=style, main_domain=it["domain"])
            calls += 1
            if ai:
                ai["topics"] = it["topics"]
                ai["region"] = it["region"]

        write_md(
            it["title"],
            it["url"],    # can√≥nica
            it["body"],
            og_image="",  # NO OG
            ai=ai,
            status="draft"
        )


    # === 1) √çndices para selecci√≥n ===
    MAX_NEW = int(os.getenv("MAX_NEW", "12"))
    GEMINI_BUDGET = int(os.getenv("GEMINI_BUDGET", "12"))

    by_region_topic = defaultdict(list)
    by_topic = defaultdict(list)
    non_big_by_topic_CO = defaultdict(list)
    for it in POOL:
        for tp in it["topics"]:
            by_topic[tp].append(it)
            by_region_topic[(it["region"], tp)].append(it)
            if it["region"] == "CO" and not it["is_big"]:
                non_big_by_topic_CO[tp].append(it)

    def pick_one(lst, used):
        for it in lst:
            if it["url"] not in used:
                used.add(it["url"])
                return it
        return None

    selected, used = [], set()

    # === 2) TU BLOQUE DE SELECCI√ìN (d√©jalo tal cual) ===
    # 1) 2 temas con m√°s publicaciones detectadas en CO (estilo cr√≠tico)
    top_CO_topics = sorted(
        [(tp, len(by_region_topic[("CO", tp)])) for tp in by_topic.keys()],
        key=lambda x: x[1], reverse=True
    )
    for tp, _n in top_CO_topics[:2]:
        it = pick_one(by_region_topic[("CO", tp)], used)
        if it: selected.append(("critico", it))
        if len(selected) >= MAX_NEW: break

    # 2) 1 tema con m√°s publicaciones en WORLD (estilo cr√≠tico)
    if len(selected) < MAX_NEW:
        top_WORLD_topics = sorted(
            [(tp, len(by_region_topic[("WORLD", tp)])) for tp in by_topic.keys()],
            key=lambda x: x[1], reverse=True
        )
        if top_WORLD_topics:
            tp, _n = top_WORLD_topics[0]
            it = pick_one(by_region_topic[("WORLD", tp)], used)
            if it: selected.append(("critico", it))

    # 3) 2 ‚Äúsilencios‚Äù en CO (no-grandes‚â•2 y grandes=0) (estilo cr√≠tico)
    if len(selected) < MAX_NEW:
        silencios = []
        for tp, lst in non_big_by_topic_CO.items():
            if len(lst) >= 2:
                big_count = sum(1 for x in by_region_topic[("CO", tp)] if x["is_big"])
                if big_count == 0:
                    silencios.append((tp, len(lst)))
        for tp, _ in sorted(silencios, key=lambda x: x[1], reverse=True)[:2]:
            it = pick_one(non_big_by_topic_CO[tp], used)
            if it: selected.append(("critico", it))
            if len(selected) >= MAX_NEW: break

    # 4) 1 Emprendimiento en CO
    if len(selected) < MAX_NEW:
        it = pick_one(by_region_topic.get(("CO","emprende"), []), used)
        if it: selected.append(("emprende", it))

    # 5) 1 Finanzas en CO
    if len(selected) < MAX_NEW:
        it = pick_one(by_region_topic.get(("CO","finanzas"), []), used)
        if it: selected.append(("finanzas", it))

    # 6) 1 Econom√≠a en CO
    if len(selected) < MAX_NEW:
        it = pick_one(by_region_topic.get(("CO","economia"), []), used)
        if it: selected.append(("economia", it))

    # 7) 1 Pagos/Fintech (CO‚ÜíLATAM‚ÜíWORLD)
    if len(selected) < MAX_NEW:
        it = (pick_one(by_region_topic.get(("CO","pagos"), []), used)
              or pick_one(by_region_topic.get(("LATAM","pagos"), []), used)
              or pick_one(by_region_topic.get(("WORLD","pagos"), []), used))
        if it: selected.append(("pagos", it))

    # 8) 1 Proyectos/Inversiones en CO
    if len(selected) < MAX_NEW:
        it = pick_one(by_region_topic.get(("CO","proyectos"), []), used)
        if it: selected.append(("proyectos", it))

    # 9) +2 extras (Tecnolog√≠a CO, Judicial CO) y relleno si faltan (prefiere CO)
    EXTRA_TARGETS = [("tecnologia","CO"), ("judicial","CO")]
    for tp, rg in EXTRA_TARGETS:
        if len(selected) >= MAX_NEW: break
        it = pick_one(by_region_topic.get((rg, tp), []), used)
        if it: selected.append(("critico", it))

    if len(selected) < MAX_NEW:
        resto = [it for it in POOL if it["url"] not in used]
        order = {"CO":0, "LATAM":1, "WORLD":2}
        resto.sort(key=lambda x: order.get(x["region"], 3))
        for it in resto:
            selected.append(("boletin", it))
            used.add(it["url"])
            if len(selected) >= MAX_NEW:
                break

    # === 3) Escritura (respeta presupuesto de IA) ===
    calls = 0
    for style, it in selected:
        ai = None
        if calls < GEMINI_BUDGET:
            ai = summarize_with_gemini(it["body"], it["url"], style=style, main_domain=it["domain"])
            calls += 1
            if ai:
                ai["topics"] = it["topics"]
                ai["region"] = it["region"]

        write_md(
            it["title"],
            it["url"],    # can√≥nica
            it["body"],
            og_image="",  # no OG
            ai=ai,
            status="draft"
        )

    GEMINI_BUDGET = int(os.getenv("GEMINI_BUDGET", "10"))  # o "12"
    calls = 0
    for style, it in selected:
        topics = it["topics"]; region = it["region"]; dom = it["domain"]
        ai = None
        if calls < GEMINI_BUDGET:
            ai = summarize_with_gemini(it["body"], it["url"], style=style, main_domain=dom)
            calls += 1
            if ai is not None:
                ai["topics"] = topics
                ai["region"]  = region

        write_md(
            it["title"],
            it["url"],      # can√≥nica
            it["body"],
            og_image="",    # no OG
            ai=ai,
            status="draft"
        )

if __name__ == "__main__":
    run()
